{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1959f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   martial-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   gender          32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hour-per-week   32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  salary          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv('adult.data') \n",
    "df_test = pd.read_csv('adult.test')\n",
    "\n",
    "# please use the files I provided;\n",
    "# i edited them manually because editing this in Python is a pain\n",
    "# and I frankly don't have the time to write that code.\n",
    "# Sorry.\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85946a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hour-per-week</th>\n",
       "      <th>salary</th>\n",
       "      <th>Federal-Gov</th>\n",
       "      <th>Local-Gov</th>\n",
       "      <th>Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>Portugal</th>\n",
       "      <th>Puerto-Rico</th>\n",
       "      <th>Scotland</th>\n",
       "      <th>South</th>\n",
       "      <th>Taiwan</th>\n",
       "      <th>Thailand</th>\n",
       "      <th>Trinadad&amp;Tobago</th>\n",
       "      <th>United-States</th>\n",
       "      <th>Vietnam</th>\n",
       "      <th>Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.044302</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.048238</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>0.138113</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>0.151068</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.221488</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>0.166404</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>0.094827</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>0.128499</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>0.187203</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.150242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    fnlwgt  education-num  capital-gain  capital-loss  hour-per-week  \\\n",
       "0      39  0.044302       0.800000      0.021740           0.0       0.397959   \n",
       "1      50  0.048238       0.800000      0.000000           0.0       0.122449   \n",
       "2      38  0.138113       0.533333      0.000000           0.0       0.397959   \n",
       "3      53  0.151068       0.400000      0.000000           0.0       0.397959   \n",
       "4      28  0.221488       0.800000      0.000000           0.0       0.397959   \n",
       "...    ..       ...            ...           ...           ...            ...   \n",
       "32556  27  0.166404       0.733333      0.000000           0.0       0.377551   \n",
       "32557  40  0.096500       0.533333      0.000000           0.0       0.397959   \n",
       "32558  58  0.094827       0.533333      0.000000           0.0       0.397959   \n",
       "32559  22  0.128499       0.533333      0.000000           0.0       0.193878   \n",
       "32560  52  0.187203       0.533333      0.150242           0.0       0.397959   \n",
       "\n",
       "       salary  Federal-Gov  Local-Gov  Never-worked  ...  Portugal  \\\n",
       "0           0          0.0        0.0           0.0  ...       0.0   \n",
       "1           0          0.0        0.0           0.0  ...       0.0   \n",
       "2           0          0.0        0.0           0.0  ...       0.0   \n",
       "3           0          0.0        0.0           0.0  ...       0.0   \n",
       "4           0          0.0        0.0           0.0  ...       0.0   \n",
       "...       ...          ...        ...           ...  ...       ...   \n",
       "32556       0          0.0        0.0           0.0  ...       0.0   \n",
       "32557       1          0.0        0.0           0.0  ...       0.0   \n",
       "32558       0          0.0        0.0           0.0  ...       0.0   \n",
       "32559       0          0.0        0.0           0.0  ...       0.0   \n",
       "32560       1          0.0        0.0           0.0  ...       0.0   \n",
       "\n",
       "       Puerto-Rico  Scotland  South  Taiwan  Thailand  Trinadad&Tobago  \\\n",
       "0              0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "1              0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "2              0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "3              0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "4              0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "...            ...       ...    ...     ...       ...              ...   \n",
       "32556          0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "32557          0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "32558          0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "32559          0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "32560          0.0       0.0    0.0     0.0       0.0              0.0   \n",
       "\n",
       "       United-States  Vietnam  Yugoslavia  \n",
       "0                1.0      0.0         0.0  \n",
       "1                1.0      0.0         0.0  \n",
       "2                1.0      0.0         0.0  \n",
       "3                1.0      0.0         0.0  \n",
       "4                0.0      0.0         0.0  \n",
       "...              ...      ...         ...  \n",
       "32556            1.0      0.0         0.0  \n",
       "32557            1.0      0.0         0.0  \n",
       "32558            1.0      0.0         0.0  \n",
       "32559            1.0      0.0         0.0  \n",
       "32560            1.0      0.0         0.0  \n",
       "\n",
       "[32561 rows x 109 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess Data\n",
    "# information based on https://archive.ics.uci.edu/ml/datasets/Adult\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# drop id - won't be needed\n",
    "# df_train = df_train.drop(['id'])\n",
    "# df_test = df_train.drop(['id'])\n",
    "\n",
    "# one hot encode workclass\n",
    "# categories: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked, ? (not known)\n",
    "df_train['workclass'] = df_train['workclass'].replace(\"?\", \"workclass_?\")\n",
    "df_test['workclass'] = df_test['workclass'].replace(\"?\", \"workclass_?\")\n",
    "\n",
    "workclassEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(workclassEncoder.fit_transform(df_train[['workclass']]).toarray(), \n",
    "                       columns=workclassEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(workclassEncoder.transform(df_test[['workclass']]).toarray(), \n",
    "                       columns=workclassEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['workclass'], axis=1)\n",
    "df_test = df_test.drop(['workclass'], axis=1)\n",
    "\n",
    "# fnlwgt  = nr of people this entry represents\n",
    "# is continuous, so scale\n",
    "fnlwgtScaler = MinMaxScaler()\n",
    "df_train['fnlwgt'] = fnlwgtScaler.fit_transform(df_train[['fnlwgt']])\n",
    "df_test['fnlwgt'] = fnlwgtScaler.transform(df_test[['fnlwgt']])\n",
    "\n",
    "# education\n",
    "educationEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(educationEncoder.fit_transform(df_train[['education']]).toarray(), \n",
    "                       columns=educationEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(educationEncoder.transform(df_test[['education']]).toarray(), \n",
    "                       columns=educationEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['education'], axis=1)\n",
    "df_test = df_test.drop(['education'], axis=1)\n",
    "\n",
    "# education-num\n",
    "# education-num: continuous.\n",
    "educationNumScaler = MinMaxScaler()\n",
    "df_train['education-num'] = educationNumScaler.fit_transform(df_train[['education-num']])\n",
    "df_test['education-num'] = educationNumScaler.transform(df_test[['education-num']])\n",
    "\n",
    "# martial-status\n",
    "# possible entries: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "martialStatusEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(martialStatusEncoder.fit_transform(df_train[['martial-status']]).toarray(), \n",
    "                       columns=martialStatusEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(martialStatusEncoder.transform(df_test[['martial-status']]).toarray(), \n",
    "                       columns=martialStatusEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['martial-status'], axis=1)\n",
    "df_test = df_test.drop(['martial-status'], axis=1)\n",
    "\n",
    "# occupation\n",
    "# possibnle entries: Tech-support, Craft-repair, Other-service, Sales, \n",
    "# Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, \n",
    "# Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces, ? (unknown)\n",
    "df_train['occupation'] = df_train['occupation'].replace(\"?\", \"occupation_?\")\n",
    "df_test['occupation'] = df_test['occupation'].replace(\"?\", \"occupation_?\")\n",
    "occupationEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(occupationEncoder.fit_transform(df_train[['occupation']]).toarray(), \n",
    "                       columns=occupationEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(occupationEncoder.transform(df_test[['occupation']]).toarray(), \n",
    "                       columns=occupationEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['occupation'], axis=1)\n",
    "df_test = df_test.drop(['occupation'], axis=1)\n",
    "\n",
    "# relationship\n",
    "# possible values: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "relationshipEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(relationshipEncoder.fit_transform(df_train[['relationship']]).toarray(), \n",
    "                       columns=relationshipEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(relationshipEncoder.transform(df_test[['relationship']]).toarray(), \n",
    "                       columns=relationshipEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['relationship'], axis=1)\n",
    "df_test = df_test.drop(['relationship'], axis=1)\n",
    "\n",
    "# race: \n",
    "# poss. values: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "raceEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(raceEncoder.fit_transform(df_train[['race']]).toarray(), \n",
    "                       columns=raceEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(raceEncoder.transform(df_test[['race']]).toarray(), \n",
    "                       columns=raceEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['race'], axis=1)\n",
    "df_test = df_test.drop(['race'], axis=1)\n",
    "\n",
    "# sex: values: Female, Male. \n",
    "sexEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(sexEncoder.fit_transform(df_train[['gender']]).toarray(), \n",
    "                       columns=sexEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(sexEncoder.transform(df_test[['gender']]).toarray(), \n",
    "                       columns=sexEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['gender'], axis=1)\n",
    "df_test = df_test.drop(['gender'], axis=1)\n",
    "\n",
    "# capital-gain: continuous.\n",
    "capitalGainScaler = MinMaxScaler()\n",
    "df_train['capital-gain'] = capitalGainScaler.fit_transform(df_train[['capital-gain']])\n",
    "df_test['capital-gain'] = capitalGainScaler.transform(df_test[['capital-gain']])\n",
    "\n",
    "# capital-loss: continuous.\n",
    "capitalLossScaler = MinMaxScaler()\n",
    "df_train['capital-loss'] = capitalLossScaler.fit_transform(df_train[['capital-loss']])\n",
    "df_test['capital-loss'] = capitalLossScaler.transform(df_test[['capital-loss']])\n",
    "\n",
    "# hours-per-week: continuous.\n",
    "hoursPerWeekScaler = MinMaxScaler()\n",
    "df_train['hour-per-week'] = hoursPerWeekScaler.fit_transform(df_train[['hour-per-week']])\n",
    "df_test['hour-per-week'] = hoursPerWeekScaler.transform(df_test[['hour-per-week']])\n",
    "\n",
    "# native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "countryEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(countryEncoder.fit_transform(df_train[['native-country']]).toarray(), \n",
    "                       columns=countryEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(countryEncoder.transform(df_test[['native-country']]).toarray(), \n",
    "                       columns=countryEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['native-country'], axis=1)\n",
    "df_test = df_test.drop(['native-country'], axis=1)\n",
    "\n",
    "# salary\n",
    "df_train['salary'] = df_train['salary'].replace('<=50K', 0)\n",
    "df_train['salary'] = df_train['salary'].replace('>50K', 1)\n",
    "df_test['salary'] = df_test['salary'].replace('<=50K.', 0)\n",
    "df_test['salary'] = df_test['salary'].replace('>50K.', 1)\n",
    "\n",
    "df_train.rename(\n",
    "    columns={\n",
    "        df_train.columns[7] : \"Federal-Gov\",\n",
    "        df_train.columns[8] : \"Local-Gov\",\n",
    "        df_train.columns[9] : \"Never-worked\",\n",
    "        df_train.columns[10] : \"Private\",\n",
    "        df_train.columns[11] : \"Self-emp-inc\",\n",
    "        df_train.columns[12] : \"Self-emp-not-inc\",\n",
    "        df_train.columns[13] : \"State-gov\",  \n",
    "        df_train.columns[14] : \"Without-pay\",\n",
    "        df_train.columns[15] : \"unknown_workclass\",\n",
    "        df_train.columns[16] : \"10th\",\n",
    "        df_train.columns[17] : \"11th\",\n",
    "        df_train.columns[18] : \"12th\",\n",
    "        df_train.columns[19] : \"1st-4th\",\n",
    "        df_train.columns[20] : \"5th-6th\",\n",
    "        df_train.columns[21] : \"7th-8th\",\n",
    "        df_train.columns[22] : \"9th\",\n",
    "        df_train.columns[23] : \"Assoc-acdm\",\n",
    "        df_train.columns[24] : \"Assoc-voc\",\n",
    "        df_train.columns[25] : \"Bachelors\",\n",
    "        df_train.columns[26] : \"Doctorate\",\n",
    "        df_train.columns[27] : \"HS-grad\",\n",
    "        df_train.columns[28] : \"Masters\",\n",
    "        df_train.columns[29] : \"Preschool\",\n",
    "        \n",
    "        df_train.columns[30] : \"Prof-school\",\n",
    "        df_train.columns[31] : \"Some-college\",\n",
    "        df_train.columns[32] : \"Divorced\",\n",
    "        df_train.columns[33] : \"Married-AF-spouse\",\n",
    "        df_train.columns[34] : \"Married-civ-spouse\",\n",
    "        df_train.columns[35] : \"Married-spouse-absent\",\n",
    "        df_train.columns[36] : \"Never-married\",\n",
    "        df_train.columns[37] : \"Separated\",\n",
    "        df_train.columns[38] : \"Widowed\",\n",
    "        df_train.columns[39] : \"Adm-clerical\",\n",
    "\n",
    "        df_train.columns[40] : \"Armed-Forces\",\n",
    "        df_train.columns[41] : \"Craft-repair\",\n",
    "        df_train.columns[42] : \"Exec-managerial\",\n",
    "        df_train.columns[43] : \"Farming-fishing\",\n",
    "        df_train.columns[44] : \"Handlers-cleaners\",\n",
    "        df_train.columns[45] : \"Machine-op-inspct\",\n",
    "        df_train.columns[46] : \"Other-service\",\n",
    "        df_train.columns[47] : \"Priv-house-serv\",\n",
    "        df_train.columns[48] : \"Prof-specialty\",\n",
    "        df_train.columns[49] : \"Protective-serv\",\n",
    "\n",
    "        df_train.columns[50] : \"Sales\",\n",
    "        df_train.columns[51] : \"Tech-support\",\n",
    "        df_train.columns[52] : \"Transport-moving\",\n",
    "        df_train.columns[53] : \"unknown_occupation\",\n",
    "        df_train.columns[54] : \"Husband\",\n",
    "        df_train.columns[55] : \"Not-in-family\",\n",
    "        df_train.columns[56] : \"Other-relative\",\n",
    "        df_train.columns[57] : \"Own-child\",\n",
    "        df_train.columns[58] : \"Unmarried\",\n",
    "        df_train.columns[59] : \"Wife\",\n",
    "\n",
    "        df_train.columns[60] : \"Amer-Indian-Eskimo\",\n",
    "        df_train.columns[61] : \"Asian-Pac-Islander\",\n",
    "        df_train.columns[62] : \"Black\",\n",
    "        df_train.columns[63] : \"Other\",\n",
    "        df_train.columns[64] : \"White\",\n",
    "        df_train.columns[65] : \"Female\",\n",
    "        df_train.columns[66] : \"Male\",\n",
    "        df_train.columns[67] : \"unknown\",\n",
    "        df_train.columns[68] : \"Cambodia\",\n",
    "        df_train.columns[69] : \"Canada\",\n",
    "\n",
    "        df_train.columns[70] : \"China\",\n",
    "        df_train.columns[71] : \"Columbia\",\n",
    "        df_train.columns[72] : \"Cuba\",\n",
    "        df_train.columns[73] : \"Dominican-Republic\",\n",
    "        df_train.columns[74] : \"Ecuador\",\n",
    "        df_train.columns[75] : \"El-Salvador\",\n",
    "        df_train.columns[76] : \"England\",\n",
    "        df_train.columns[77] : \"France\",\n",
    "        df_train.columns[78] : \"Germany\",\n",
    "        df_train.columns[79] : \"Greece\",\n",
    "\n",
    "        df_train.columns[80] : \"Guatemala\",\n",
    "        df_train.columns[81] : \"Haiti\",\n",
    "        df_train.columns[82] : \"Holand-Netherlands\",\n",
    "        df_train.columns[83] : \"Honduras\",\n",
    "        df_train.columns[84] : \"Hong\",\n",
    "        df_train.columns[85] : \"Hungary\",\n",
    "        df_train.columns[86] : \"India\",\n",
    "        df_train.columns[87] : \"Iran\",\n",
    "        df_train.columns[88] : \"Ireland\",\n",
    "        df_train.columns[89] : \"Italy\",\n",
    "\n",
    "        df_train.columns[90] : \"Jamaica\",\n",
    "        df_train.columns[91] : \"Japan\",\n",
    "        df_train.columns[92] : \"Laos\",\n",
    "        df_train.columns[93] : \"Mexico\",\n",
    "        df_train.columns[94] : \"Nicaragua\",\n",
    "        df_train.columns[95] : \"Outlying-US(Guam-USVI-etc)\",\n",
    "        df_train.columns[96] : \"Peru\",\n",
    "        df_train.columns[97] : \"Philippines\",\n",
    "        df_train.columns[98] : \"Poland\",\n",
    "        df_train.columns[99] : \"Portugal\",\n",
    "        \n",
    "        df_train.columns[100] : \"Puerto-Rico\",\n",
    "        df_train.columns[101] : \"Scotland\",\n",
    "        df_train.columns[102] : \"South\",\n",
    "        df_train.columns[103] : \"Taiwan\",\n",
    "        df_train.columns[104] : \"Thailand\",\n",
    "        df_train.columns[105] : \"Trinadad&Tobago\",\n",
    "        df_train.columns[106] : \"United-States\",\n",
    "        df_train.columns[107] : \"Vietnam\",\n",
    "        df_train.columns[108] : \"Yugoslavia\"        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0556df5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 id\n",
      "1 fnlwgt\n",
      "2 education-num\n",
      "3 capital-gain\n",
      "4 capital-loss\n",
      "5 hour-per-week\n",
      "6 salary\n",
      "7 ('Federal-gov',)\n",
      "8 ('Local-gov',)\n",
      "9 ('Never-worked',)\n",
      "10 ('Private',)\n",
      "11 ('Self-emp-inc',)\n",
      "12 ('Self-emp-not-inc',)\n",
      "13 ('State-gov',)\n",
      "14 ('Without-pay',)\n",
      "15 ('workclass_?',)\n",
      "16 ('10th',)\n",
      "17 ('11th',)\n",
      "18 ('12th',)\n",
      "19 ('1st-4th',)\n",
      "20 ('5th-6th',)\n",
      "21 ('7th-8th',)\n",
      "22 ('9th',)\n",
      "23 ('Assoc-acdm',)\n",
      "24 ('Assoc-voc',)\n",
      "25 ('Bachelors',)\n",
      "26 ('Doctorate',)\n",
      "27 ('HS-grad',)\n",
      "28 ('Masters',)\n",
      "29 ('Preschool',)\n",
      "30 ('Prof-school',)\n",
      "31 ('Some-college',)\n",
      "32 ('Divorced',)\n",
      "33 ('Married-AF-spouse',)\n",
      "34 ('Married-civ-spouse',)\n",
      "35 ('Married-spouse-absent',)\n",
      "36 ('Never-married',)\n",
      "37 ('Separated',)\n",
      "38 ('Widowed',)\n",
      "39 ('Adm-clerical',)\n",
      "40 ('Armed-Forces',)\n",
      "41 ('Craft-repair',)\n",
      "42 ('Exec-managerial',)\n",
      "43 ('Farming-fishing',)\n",
      "44 ('Handlers-cleaners',)\n",
      "45 ('Machine-op-inspct',)\n",
      "46 ('Other-service',)\n",
      "47 ('Priv-house-serv',)\n",
      "48 ('Prof-specialty',)\n",
      "49 ('Protective-serv',)\n",
      "50 ('Sales',)\n",
      "51 ('Tech-support',)\n",
      "52 ('Transport-moving',)\n",
      "53 ('occupation_?',)\n",
      "54 ('Husband',)\n",
      "55 ('Not-in-family',)\n",
      "56 ('Other-relative',)\n",
      "57 ('Own-child',)\n",
      "58 ('Unmarried',)\n",
      "59 ('Wife',)\n",
      "60 ('Amer-Indian-Eskimo',)\n",
      "61 ('Asian-Pac-Islander',)\n",
      "62 ('Black',)\n",
      "63 ('Other',)\n",
      "64 ('White',)\n",
      "65 ('Female',)\n",
      "66 ('Male',)\n",
      "67 ('?',)\n",
      "68 ('Cambodia',)\n",
      "69 ('Canada',)\n",
      "70 ('China',)\n",
      "71 ('Columbia',)\n",
      "72 ('Cuba',)\n",
      "73 ('Dominican-Republic',)\n",
      "74 ('Ecuador',)\n",
      "75 ('El-Salvador',)\n",
      "76 ('England',)\n",
      "77 ('France',)\n",
      "78 ('Germany',)\n",
      "79 ('Greece',)\n",
      "80 ('Guatemala',)\n",
      "81 ('Haiti',)\n",
      "82 ('Holand-Netherlands',)\n",
      "83 ('Honduras',)\n",
      "84 ('Hong',)\n",
      "85 ('Hungary',)\n",
      "86 ('India',)\n",
      "87 ('Iran',)\n",
      "88 ('Ireland',)\n",
      "89 ('Italy',)\n",
      "90 ('Jamaica',)\n",
      "91 ('Japan',)\n",
      "92 ('Laos',)\n",
      "93 ('Mexico',)\n",
      "94 ('Nicaragua',)\n",
      "95 ('Outlying-US(Guam-USVI-etc)',)\n",
      "96 ('Peru',)\n",
      "97 ('Philippines',)\n",
      "98 ('Poland',)\n",
      "99 ('Portugal',)\n",
      "100 ('Puerto-Rico',)\n",
      "101 ('Scotland',)\n",
      "102 ('South',)\n",
      "103 ('Taiwan',)\n",
      "104 ('Thailand',)\n",
      "105 ('Trinadad&Tobago',)\n",
      "106 ('United-States',)\n",
      "107 ('Vietnam',)\n",
      "108 ('Yugoslavia',)\n"
     ]
    }
   ],
   "source": [
    "for i in range (109):\n",
    "    print(i,  df_train.columns[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5471222",
   "metadata": {},
   "source": [
    "# Explanation on Preprocessing\n",
    "*Based on exploratory data analysis, discuss what preprocessing that you need to do before classification, and provide evidence and justifications.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7e4245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 1]\n",
      "Naive Bayes\n",
      "[0 1 1 ... 1 0 1]\n",
      "SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 1]\n",
      "Decision Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 0 0]\n",
      "Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 0 1]\n",
      "AdaBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "Gradient Boosting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 1]\n",
      "linear discriminant analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 1]\n",
      "multi-layer perceptron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 1]\n",
      "logistic regression\n",
      "[0 0 0 ... 1 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# (1) kNN, (2) naive Bayes, (3) SVM, (4) decision tree, \n",
    "# (5) random forest, (6) AdaBoost, (7) gradient Boosting, \n",
    "# (8) linear discriminant analysis, (9) multi-layer perceptron, and\n",
    "# (10) logistic regression.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB # this is the first result? hope it's right? sorry if not?\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "models = [\"Models\", KNeighborsClassifier(), GaussianNB(), SVC( probability=True),\n",
    "         DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "         GradientBoostingClassifier(), LinearDiscriminantAnalysis(), \n",
    "         MLPClassifier(), LogisticRegression()]\n",
    "\n",
    "names = [\"Models\", \"KNN\", \"Naive Bayes\", \"SVC\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
    "         \"Gradient Boosting\", \"linear discriminant analysis\", \n",
    "         \"multi-layer perceptron\", \"logistic regression\"\n",
    "        ]\n",
    "\n",
    "accuracy = [\"Accuracy\"]\n",
    "recall = [\"Recall Positive\"]\n",
    "recallNegative = [\"Recall Negative\"]\n",
    "precision = [\"Precision\"]\n",
    "f1 = [\"F1\"]\n",
    "aucList = [\"AUC\"]\n",
    "\n",
    "y_train = df_train['salary']\n",
    "x_train = df_train.drop('salary', axis=1)\n",
    "\n",
    "y_test = df_test['salary']\n",
    "x_test = df_test.drop('salary', axis=1)\n",
    "\n",
    "for i in range (1, 11): \n",
    "    model = models[i]\n",
    "    name = names[i]\n",
    "    \n",
    "    print(name)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_prob = model.predict_proba(x_test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    recall.append( recall_score(y_test, y_pred))\n",
    "    recallNegative.append( recall_score(y_test, y_pred, pos_label=0))\n",
    "    precision.append( precision_score(y_test, y_pred))\n",
    "    f1.append( f1_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1], pos_label=1)\n",
    "    aucList.append(auc(fpr, tpr) )\n",
    "    \n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81faf5c",
   "metadata": {},
   "source": [
    "*Report the results (keep 2 decimals) of all the 10 classification algorithms on the given test data in terms of classification accuracy, precision, recall, F1-score, and AUC. You should report them in a table.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18c5a31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Names          </td><td>KNN </td><td>Naive Bayes</td><td>SVC </td><td>Decision Tree</td><td>Random Forest</td><td>AdaBoost</td><td>Gradient Boosting</td><td>linear discriminant analysis</td><td>multi-layer perceptron</td><td>logistic regression</td></tr>\n",
       "<tr><td>Accuracy       </td><td>0.82</td><td>0.6        </td><td>0.83</td><td>0.81         </td><td>0.85         </td><td>0.86    </td><td>0.87             </td><td>0.84                        </td><td>0.85                  </td><td>0.84               </td></tr>\n",
       "<tr><td>Recall Positive</td><td>0.55</td><td>0.93       </td><td>0.46</td><td>0.62         </td><td>0.61         </td><td>0.61    </td><td>0.61             </td><td>0.56                        </td><td>0.54                  </td><td>0.57               </td></tr>\n",
       "<tr><td>Recall Negative</td><td>0.9 </td><td>0.5        </td><td>0.95</td><td>0.87         </td><td>0.93         </td><td>0.94    </td><td>0.95             </td><td>0.93                        </td><td>0.95                  </td><td>0.93               </td></tr>\n",
       "<tr><td>Precision      </td><td>0.63</td><td>0.36       </td><td>0.75</td><td>0.6          </td><td>0.72         </td><td>0.75    </td><td>0.8              </td><td>0.72                        </td><td>0.77                  </td><td>0.7                </td></tr>\n",
       "<tr><td>F1             </td><td>0.59</td><td>0.52       </td><td>0.57</td><td>0.61         </td><td>0.66         </td><td>0.67    </td><td>0.69             </td><td>0.63                        </td><td>0.63                  </td><td>0.63               </td></tr>\n",
       "<tr><td>AUC            </td><td>0.83</td><td>0.81       </td><td>0.88</td><td>0.75         </td><td>0.9          </td><td>0.91    </td><td>0.92             </td><td>0.89                        </td><td>0.91                  </td><td>0.89               </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<tbody>\\n<tr><td>Names          </td><td>KNN </td><td>Naive Bayes</td><td>SVC </td><td>Decision Tree</td><td>Random Forest</td><td>AdaBoost</td><td>Gradient Boosting</td><td>linear discriminant analysis</td><td>multi-layer perceptron</td><td>logistic regression</td></tr>\\n<tr><td>Accuracy       </td><td>0.82</td><td>0.6        </td><td>0.83</td><td>0.81         </td><td>0.85         </td><td>0.86    </td><td>0.87             </td><td>0.84                        </td><td>0.85                  </td><td>0.84               </td></tr>\\n<tr><td>Recall Positive</td><td>0.55</td><td>0.93       </td><td>0.46</td><td>0.62         </td><td>0.61         </td><td>0.61    </td><td>0.61             </td><td>0.56                        </td><td>0.54                  </td><td>0.57               </td></tr>\\n<tr><td>Recall Negative</td><td>0.9 </td><td>0.5        </td><td>0.95</td><td>0.87         </td><td>0.93         </td><td>0.94    </td><td>0.95             </td><td>0.93                        </td><td>0.95                  </td><td>0.93               </td></tr>\\n<tr><td>Precision      </td><td>0.63</td><td>0.36       </td><td>0.75</td><td>0.6          </td><td>0.72         </td><td>0.75    </td><td>0.8              </td><td>0.72                        </td><td>0.77                  </td><td>0.7                </td></tr>\\n<tr><td>F1             </td><td>0.59</td><td>0.52       </td><td>0.57</td><td>0.61         </td><td>0.66         </td><td>0.67    </td><td>0.69             </td><td>0.63                        </td><td>0.63                  </td><td>0.63               </td></tr>\\n<tr><td>AUC            </td><td>0.83</td><td>0.81       </td><td>0.88</td><td>0.75         </td><td>0.9          </td><td>0.91    </td><td>0.92             </td><td>0.89                        </td><td>0.91                  </td><td>0.89               </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tabulate\n",
    "\n",
    "\n",
    "header = names\n",
    "data = [header, accuracy, recall, recallNegative, precision, f1, aucList]\n",
    "\n",
    "for i in range (1, len (data)):\n",
    "    for j in range (1, len (data[i])):\n",
    "        data[i][j] = round(data[i][j], 2)\n",
    "table = tabulate.tabulate(data, tablefmt='html')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bb52c",
   "metadata": {},
   "source": [
    "*Find the two best algorithms according to each of the four performance metrics, Are they the same? Explain why.*\n",
    "\n",
    "Accuracy: AdaBoost, Gradient Boosting\n",
    "Recall (Positive): Naive Bayed, Decision Tree\n",
    "Recall (Negative): SVC, Gradient Boosting, Multi-Layer Perceptron\n",
    "Precision: Multi-Layer Perceptron, SVC\n",
    "F1: Gradient Boosting, AdaBoost\n",
    "AUC: Gradient Boosting, AdaBoost, Multi-Layer Perceptron\n",
    "\n",
    "The best performing algorithms across those are AdaBoost, Gradient Boosting and Multi-Layer Perceptrons. This may just be because those algorithms perform better on this data set with the given parameters (the default ones), but it also just shows a general strength for them. Logistic Regression certainly doesn't do well with the default parameters, because it never manages to converge in the first place, explaining its lower performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
