{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1959f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   martial-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   gender          32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hour-per-week   32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  salary          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv('adult.data') \n",
    "df_test = pd.read_csv('adult.test')\n",
    "\n",
    "# please use the files I provided;\n",
    "# i edited them manually because editing this in Python is a pain\n",
    "# and I frankly don't have the time to write that code.\n",
    "# Sorry.\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85946a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "# information based on https://archive.ics.uci.edu/ml/datasets/Adult\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# drop id - won't be needed\n",
    "# df_train = df_train.drop(['id'])\n",
    "# df_test = df_train.drop(['id'])\n",
    "\n",
    "# one hot encode workclass\n",
    "# categories: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked, ? (not known)\n",
    "df_train['workclass'] = df_train['workclass'].replace(\"?\", \"workclass_?\")\n",
    "df_test['workclass'] = df_test['workclass'].replace(\"?\", \"workclass_?\")\n",
    "\n",
    "workclassEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(workclassEncoder.fit_transform(df_train[['workclass']]).toarray(), \n",
    "                       columns=workclassEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(workclassEncoder.transform(df_test[['workclass']]).toarray(), \n",
    "                       columns=workclassEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['workclass'], axis=1)\n",
    "df_test = df_test.drop(['workclass'], axis=1)\n",
    "\n",
    "# fnlwgt  = nr of people this entry represents\n",
    "# is continuous, so scale\n",
    "fnlwgtScaler = MinMaxScaler()\n",
    "df_train['fnlwgt'] = fnlwgtScaler.fit_transform(df_train[['fnlwgt']])\n",
    "df_test['fnlwgt'] = fnlwgtScaler.transform(df_test[['fnlwgt']])\n",
    "\n",
    "# education\n",
    "educationEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(educationEncoder.fit_transform(df_train[['education']]).toarray(), \n",
    "                       columns=educationEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(educationEncoder.transform(df_test[['education']]).toarray(), \n",
    "                       columns=educationEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['education'], axis=1)\n",
    "df_test = df_test.drop(['education'], axis=1)\n",
    "\n",
    "# education-num\n",
    "# education-num: continuous.\n",
    "educationNumScaler = MinMaxScaler()\n",
    "df_train['education-num'] = educationNumScaler.fit_transform(df_train[['education-num']])\n",
    "df_test['education-num'] = educationNumScaler.transform(df_test[['education-num']])\n",
    "\n",
    "# martial-status\n",
    "# possible entries: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "martialStatusEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(martialStatusEncoder.fit_transform(df_train[['martial-status']]).toarray(), \n",
    "                       columns=martialStatusEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(martialStatusEncoder.transform(df_test[['martial-status']]).toarray(), \n",
    "                       columns=martialStatusEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['martial-status'], axis=1)\n",
    "df_test = df_test.drop(['martial-status'], axis=1)\n",
    "\n",
    "# occupation\n",
    "# possibnle entries: Tech-support, Craft-repair, Other-service, Sales, \n",
    "# Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, \n",
    "# Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces, ? (unknown)\n",
    "df_train['occupation'] = df_train['occupation'].replace(\"?\", \"occupation_?\")\n",
    "df_test['occupation'] = df_test['occupation'].replace(\"?\", \"occupation_?\")\n",
    "occupationEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(occupationEncoder.fit_transform(df_train[['occupation']]).toarray(), \n",
    "                       columns=occupationEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(occupationEncoder.transform(df_test[['occupation']]).toarray(), \n",
    "                       columns=occupationEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['occupation'], axis=1)\n",
    "df_test = df_test.drop(['occupation'], axis=1)\n",
    "\n",
    "# relationship\n",
    "# possible values: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "relationshipEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(relationshipEncoder.fit_transform(df_train[['relationship']]).toarray(), \n",
    "                       columns=relationshipEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(relationshipEncoder.transform(df_test[['relationship']]).toarray(), \n",
    "                       columns=relationshipEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['relationship'], axis=1)\n",
    "df_test = df_test.drop(['relationship'], axis=1)\n",
    "\n",
    "# race: \n",
    "# poss. values: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "raceEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(raceEncoder.fit_transform(df_train[['race']]).toarray(), \n",
    "                       columns=raceEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(raceEncoder.transform(df_test[['race']]).toarray(), \n",
    "                       columns=raceEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['race'], axis=1)\n",
    "df_test = df_test.drop(['race'], axis=1)\n",
    "\n",
    "# sex: values: Female, Male. \n",
    "sexEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(sexEncoder.fit_transform(df_train[['gender']]).toarray(), \n",
    "                       columns=sexEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(sexEncoder.transform(df_test[['gender']]).toarray(), \n",
    "                       columns=sexEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['gender'], axis=1)\n",
    "df_test = df_test.drop(['gender'], axis=1)\n",
    "\n",
    "# capital-gain: continuous.\n",
    "capitalGainScaler = MinMaxScaler()\n",
    "df_train['capital-gain'] = capitalGainScaler.fit_transform(df_train[['capital-gain']])\n",
    "df_test['capital-gain'] = capitalGainScaler.transform(df_test[['capital-gain']])\n",
    "\n",
    "# capital-loss: continuous.\n",
    "capitalLossScaler = MinMaxScaler()\n",
    "df_train['capital-loss'] = capitalLossScaler.fit_transform(df_train[['capital-loss']])\n",
    "df_test['capital-loss'] = capitalLossScaler.transform(df_test[['capital-loss']])\n",
    "\n",
    "# hours-per-week: continuous.\n",
    "hoursPerWeekScaler = MinMaxScaler()\n",
    "df_train['hour-per-week'] = hoursPerWeekScaler.fit_transform(df_train[['hour-per-week']])\n",
    "df_test['hour-per-week'] = hoursPerWeekScaler.transform(df_test[['hour-per-week']])\n",
    "\n",
    "# native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "countryEncoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded = pd.DataFrame(countryEncoder.fit_transform(df_train[['native-country']]).toarray(), \n",
    "                       columns=countryEncoder.categories_)\n",
    "df_train = df_train.join(encoded)\n",
    "encoded = pd.DataFrame(countryEncoder.transform(df_test[['native-country']]).toarray(), \n",
    "                       columns=countryEncoder.categories_)\n",
    "df_test = df_test.join(encoded)\n",
    "\n",
    "df_train = df_train.drop(['native-country'], axis=1)\n",
    "df_test = df_test.drop(['native-country'], axis=1)\n",
    "\n",
    "# salary\n",
    "df_train['salary'] = df_train['salary'].replace(\"<=50k\", 0)\n",
    "df_train['salary'] = df_train['salary'].replace(\">50k\", 1)\n",
    "df_test['salary'] = df_test['salary'].replace(\"<=50k\", 0)\n",
    "df_test['salary'] = df_test['salary'].replace(\">50k\", 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f471c83",
   "metadata": {},
   "source": [
    "# Explanation on Preprocessing\n",
    "* Based on exploratory data analysis, discuss what preprocessing that you need to do before classification, and provide evidence and justifications.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b4282bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "# (1) kNN, (2) naive Bayes, (3) SVM, (4) decision tree, \n",
    "# (5) random forest, (6) AdaBoost, (7) gradient Boosting, \n",
    "# (8) linear discriminant analysis, (9) multi-layer perceptron, and\n",
    "# (10) logistic regression.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB # this is the first result? hope it's right? sorry if not?\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "models = [\"Models\", KNeighborsClassifier(), GaussianNB(), SVC(),\n",
    "         DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "         GradientBoostingClassifier(), LinearDiscriminantAnalysis(), \n",
    "         MLPClassifier(), LogisticRegression()]\n",
    "\n",
    "names = [\"Names\", \"KNN\", \"Naive Bayed\", \"SVC\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
    "         \"Gradient Boosting\", \"linear discriminant analysis\", \n",
    "         \"multi-layer perceptron\", \"logistic regression\"\n",
    "        ]\n",
    "\n",
    "accuracy = [\"Accuracy\"]\n",
    "recall = [\"Recall Positive\"]\n",
    "recallNegative = [\"Recall Negative\"]\n",
    "precision = [\"Precision\"]\n",
    "f1 = [\"F1\"]\n",
    "aucList = [\"AUC\"]\n",
    "\n",
    "y_train = df_train['salary']\n",
    "x_train = df_train.drop(['salary'])\n",
    "\n",
    "y_test = df_test['salary']\n",
    "x_test = df_test.drop(['salary'])\n",
    "\n",
    "for i in range (1, 11): \n",
    "    model = models[i]\n",
    "    name = name[i]\n",
    "    \n",
    "    print(name)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_prob = model.predict_proba(x_test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    recall.append( recall_score(y_test, y_pred))\n",
    "    recallNegative.append( recall_score(y_test, y_pred, pos_label=0))\n",
    "    precision.append( precision_score(y_test, y_pred))\n",
    "    f1.append( f1_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1], pos_label=1)\n",
    "    aucList.append(auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2befeb4",
   "metadata": {},
   "source": [
    "* Report the results (keep 2 decimals) of all the 10 classification algorithms on the given test data in terms of classification accuracy, precision, recall, F1-score, and AUC. You should report them in a table.* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
